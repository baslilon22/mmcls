2023-11-13 15:55:05,863 - mmcls - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.17 | packaged by conda-forge | (default, Jun 16 2023, 07:06:00) [GCC 11.4.0]
CUDA available: True
GPU 0,1: NVIDIA GeForce RTX 3090
CUDA_HOME: /usr/local/cuda-11.6
NVCC: Build cuda_11.6.r11.6/compiler.31057947_0
GCC: gcc (GCC) 7.3.0
PyTorch: 1.8.0+cu111
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.9.0+cu111
OpenCV: 4.8.0
MMCV: 1.4.2
MMCV Compiler: n/a
MMCV CUDA Compiler: n/a
MMClassification: 0.24.0+
------------------------------------------------------------

2023-11-13 15:55:05,864 - mmcls - INFO - Distributed training: True
2023-11-13 15:55:06,361 - mmcls - INFO - Config:
train_data_root = '/data4/lj/forgery_Detect/Forgery_train_20231106'
test_data_root = '/data4/lj/forgery_Detect/Forgery_test_20231106'
work_dir = 'work_dirs/swin_tiny/FP/11_13'
num_classes = 2
batch_size = 70
max_epochs = 200
interval_save = 50
model = dict(
    type='ImageClassifier',
    backbone=dict(
        type='SwinTransformer',
        arch='tiny',
        init_cfg=dict(
            type='Pretrained',
            checkpoint=
            'pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth',
            prefix='backbone'),
        img_size=224,
        drop_path_rate=0.2),
    neck=dict(type='GlobalAveragePooling'),
    head=dict(
        type='LinearClsHead',
        num_classes=2,
        in_channels=768,
        init_cfg=None,
        loss=dict(type='FocalLoss', gamma=2.0, alpha=0.25, reduction='mean'),
        cal_acc=False),
    init_cfg=[
        dict(type='TruncNormal', layer='Linear', std=0.02, bias=0.0),
        dict(type='Constant', layer='LayerNorm', val=1.0, bias=0.0)
    ])
dataset_type = 'CustomDataset'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='Rotate', angle=8.0, interpolation='bicubic'),
    dict(
        type='Rotate',
        angle=90.0,
        interpolation='bicubic',
        prob=0.1,
        random_negative_prob=0.5),
    dict(
        type='Rotate',
        angle=180.0,
        interpolation='bicubic',
        prob=0.1,
        random_negative_prob=0.5),
    dict(type='Resize', size=224),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='ImageToTensor', keys=['img']),
    dict(type='ToTensor', keys=['gt_label']),
    dict(type='Collect', keys=['img', 'gt_label'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='Resize', size=224),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='ImageToTensor', keys=['img']),
    dict(type='Collect', keys=['img'])
]
data = dict(
    samples_per_gpu=190,
    workers_per_gpu=4,
    train=dict(
        type='CustomDataset',
        data_prefix='/data4/lj/forgery_Detect/Forgery_train_20231113',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='Rotate', angle=8.0, interpolation='bicubic'),
            dict(
                type='Rotate',
                angle=90.0,
                interpolation='bicubic',
                prob=0.1,
                random_negative_prob=0.5),
            dict(
                type='Rotate',
                angle=180.0,
                interpolation='bicubic',
                prob=0.1,
                random_negative_prob=0.5),
            dict(type='Resize', size=224),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='ToTensor', keys=['gt_label']),
            dict(type='Collect', keys=['img', 'gt_label'])
        ]),
    val=dict(
        type='CustomDataset',
        data_prefix='/data4/lj/forgery_Detect/Forgery_test_20231113',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='Resize', size=224),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ]),
    test=dict(
        type='CustomDataset',
        data_prefix='/data4/lj/forgery_Detect/Forgery_test_20231113',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='Resize', size=224),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ]))
evaluation = dict(interval=10, metric='accuracy')
paramwise_cfg = dict(
    norm_decay_mult=0.0,
    bias_decay_mult=0.0,
    custom_keys=dict({
        '.absolute_pos_embed': dict(decay_mult=0.0),
        '.relative_position_bias_table': dict(decay_mult=0.0)
    }))
optimizer = dict(
    type='AdamW',
    lr=0.001,
    weight_decay=0.01,
    eps=1e-08,
    betas=(0.9, 0.999),
    paramwise_cfg=dict(
        norm_decay_mult=0.0,
        bias_decay_mult=0.0,
        custom_keys=dict({
            '.absolute_pos_embed': dict(decay_mult=0.0),
            '.relative_position_bias_table': dict(decay_mult=0.0)
        })))
optimizer_config = dict(grad_clip=dict(max_norm=5.0))
lr_config = dict(
    policy='CosineAnnealing',
    by_epoch=False,
    min_lr_ratio=0.005,
    warmup='linear',
    warmup_ratio=0.001,
    warmup_iters=20,
    warmup_by_epoch=True)
runner = dict(type='EpochBasedRunner', max_epochs=200)
checkpoint_config = dict(interval=10, save_optimizer=True)
log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
gpu_ids = range(0, 2)

2023-11-13 15:56:05,058 - mmcls - INFO - Set random seed to 320190695, deterministic: False
2023-11-13 15:56:05,767 - mmcls - INFO - initialize ImageClassifier with init_cfg [{'type': 'TruncNormal', 'layer': 'Linear', 'std': 0.02, 'bias': 0.0}, {'type': 'Constant', 'layer': 'LayerNorm', 'val': 1.0, 'bias': 0.0}]
2023-11-13 15:56:07,475 - mmcls - INFO - initialize SwinTransformer with init_cfg {'type': 'Pretrained', 'checkpoint': 'pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth', 'prefix': 'backbone'}
Name of parameter - Initialization information

backbone.patch_embed.projection.weight - torch.Size([96, 3, 4, 4]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.patch_embed.projection.bias - torch.Size([96]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.patch_embed.norm.weight - torch.Size([96]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.patch_embed.norm.bias - torch.Size([96]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.0.norm1.weight - torch.Size([96]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.0.norm1.bias - torch.Size([96]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 3]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.0.attn.w_msa.qkv.weight - torch.Size([288, 96]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.0.attn.w_msa.qkv.bias - torch.Size([288]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.0.attn.w_msa.proj.weight - torch.Size([96, 96]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.0.attn.w_msa.proj.bias - torch.Size([96]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.0.norm2.weight - torch.Size([96]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.0.norm2.bias - torch.Size([96]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.0.ffn.layers.0.0.weight - torch.Size([384, 96]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.0.ffn.layers.0.0.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.0.ffn.layers.1.weight - torch.Size([96, 384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.0.ffn.layers.1.bias - torch.Size([96]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.1.norm1.weight - torch.Size([96]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.1.norm1.bias - torch.Size([96]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 3]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.1.attn.w_msa.qkv.weight - torch.Size([288, 96]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.1.attn.w_msa.qkv.bias - torch.Size([288]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.1.attn.w_msa.proj.weight - torch.Size([96, 96]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.1.attn.w_msa.proj.bias - torch.Size([96]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.1.norm2.weight - torch.Size([96]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.1.norm2.bias - torch.Size([96]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.1.ffn.layers.0.0.weight - torch.Size([384, 96]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.1.ffn.layers.0.0.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.1.ffn.layers.1.weight - torch.Size([96, 384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.blocks.1.ffn.layers.1.bias - torch.Size([96]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.downsample.norm.weight - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.downsample.norm.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.0.downsample.reduction.weight - torch.Size([192, 384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.0.norm1.weight - torch.Size([192]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.0.norm1.bias - torch.Size([192]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 6]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.0.attn.w_msa.qkv.weight - torch.Size([576, 192]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.0.attn.w_msa.qkv.bias - torch.Size([576]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.0.attn.w_msa.proj.weight - torch.Size([192, 192]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.0.attn.w_msa.proj.bias - torch.Size([192]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.0.norm2.weight - torch.Size([192]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.0.norm2.bias - torch.Size([192]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.0.ffn.layers.0.0.weight - torch.Size([768, 192]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.0.ffn.layers.0.0.bias - torch.Size([768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.0.ffn.layers.1.weight - torch.Size([192, 768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.0.ffn.layers.1.bias - torch.Size([192]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.1.norm1.weight - torch.Size([192]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.1.norm1.bias - torch.Size([192]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 6]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.1.attn.w_msa.qkv.weight - torch.Size([576, 192]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.1.attn.w_msa.qkv.bias - torch.Size([576]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.1.attn.w_msa.proj.weight - torch.Size([192, 192]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.1.attn.w_msa.proj.bias - torch.Size([192]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.1.norm2.weight - torch.Size([192]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.1.norm2.bias - torch.Size([192]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.1.ffn.layers.0.0.weight - torch.Size([768, 192]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.1.ffn.layers.0.0.bias - torch.Size([768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.1.ffn.layers.1.weight - torch.Size([192, 768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.blocks.1.ffn.layers.1.bias - torch.Size([192]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.downsample.norm.weight - torch.Size([768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.downsample.norm.bias - torch.Size([768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.1.downsample.reduction.weight - torch.Size([384, 768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.0.norm1.weight - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.0.norm1.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.0.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.0.attn.w_msa.qkv.bias - torch.Size([1152]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.0.attn.w_msa.proj.weight - torch.Size([384, 384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.0.attn.w_msa.proj.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.0.norm2.weight - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.0.norm2.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.0.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.0.ffn.layers.0.0.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.0.ffn.layers.1.weight - torch.Size([384, 1536]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.0.ffn.layers.1.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.1.norm1.weight - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.1.norm1.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.1.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.1.attn.w_msa.qkv.bias - torch.Size([1152]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.1.attn.w_msa.proj.weight - torch.Size([384, 384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.1.attn.w_msa.proj.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.1.norm2.weight - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.1.norm2.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.1.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.1.ffn.layers.0.0.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.1.ffn.layers.1.weight - torch.Size([384, 1536]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.1.ffn.layers.1.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.2.norm1.weight - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.2.norm1.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.2.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.2.attn.w_msa.qkv.bias - torch.Size([1152]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.2.attn.w_msa.proj.weight - torch.Size([384, 384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.2.attn.w_msa.proj.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.2.norm2.weight - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.2.norm2.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.2.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.2.ffn.layers.0.0.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.2.ffn.layers.1.weight - torch.Size([384, 1536]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.2.ffn.layers.1.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.3.norm1.weight - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.3.norm1.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.3.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.3.attn.w_msa.qkv.bias - torch.Size([1152]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.3.attn.w_msa.proj.weight - torch.Size([384, 384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.3.attn.w_msa.proj.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.3.norm2.weight - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.3.norm2.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.3.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.3.ffn.layers.0.0.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.3.ffn.layers.1.weight - torch.Size([384, 1536]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.3.ffn.layers.1.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.4.norm1.weight - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.4.norm1.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.4.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.4.attn.w_msa.qkv.bias - torch.Size([1152]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.4.attn.w_msa.proj.weight - torch.Size([384, 384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.4.attn.w_msa.proj.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.4.norm2.weight - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.4.norm2.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.4.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.4.ffn.layers.0.0.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.4.ffn.layers.1.weight - torch.Size([384, 1536]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.4.ffn.layers.1.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.5.norm1.weight - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.5.norm1.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.5.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.5.attn.w_msa.qkv.bias - torch.Size([1152]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.5.attn.w_msa.proj.weight - torch.Size([384, 384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.5.attn.w_msa.proj.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.5.norm2.weight - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.5.norm2.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.5.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.5.ffn.layers.0.0.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.5.ffn.layers.1.weight - torch.Size([384, 1536]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.blocks.5.ffn.layers.1.bias - torch.Size([384]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.downsample.norm.weight - torch.Size([1536]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.downsample.norm.bias - torch.Size([1536]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.2.downsample.reduction.weight - torch.Size([768, 1536]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.0.norm1.weight - torch.Size([768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.0.norm1.bias - torch.Size([768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.0.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.0.attn.w_msa.qkv.bias - torch.Size([2304]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.0.attn.w_msa.proj.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.0.attn.w_msa.proj.bias - torch.Size([768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.0.norm2.weight - torch.Size([768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.0.norm2.bias - torch.Size([768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.0.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.0.ffn.layers.0.0.bias - torch.Size([3072]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.0.ffn.layers.1.weight - torch.Size([768, 3072]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.0.ffn.layers.1.bias - torch.Size([768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.1.norm1.weight - torch.Size([768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.1.norm1.bias - torch.Size([768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.1.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.1.attn.w_msa.qkv.bias - torch.Size([2304]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.1.attn.w_msa.proj.weight - torch.Size([768, 768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.1.attn.w_msa.proj.bias - torch.Size([768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.1.norm2.weight - torch.Size([768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.1.norm2.bias - torch.Size([768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.1.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.1.ffn.layers.0.0.bias - torch.Size([3072]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.1.ffn.layers.1.weight - torch.Size([768, 3072]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.stages.3.blocks.1.ffn.layers.1.bias - torch.Size([768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.norm3.weight - torch.Size([768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

backbone.norm3.bias - torch.Size([768]): 
PretrainedInit: load from pretrain_model/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth 

head.fc.weight - torch.Size([2, 768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

head.fc.bias - torch.Size([2]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 
2023-11-13 15:56:32,034 - mmcls - INFO - Start running, host: root@8F-02-37u-AItest29, work_dir: /data4/lj/Classification/1.7/work_dirs/swin_tiny/FP/11_13
2023-11-13 15:56:32,034 - mmcls - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) DistOptimizerHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2023-11-13 15:56:32,035 - mmcls - INFO - workflow: [('train', 1)], max: 200 epochs
2023-11-13 15:56:32,035 - mmcls - INFO - Checkpoints will be saved to /data4/lj/Classification/1.7/work_dirs/swin_tiny/FP/11_13 by HardDiskBackend.
2023-11-13 16:02:16,289 - mmcls - INFO - Epoch [1][50/195]	lr: 1.355e-05, eta: 3 days, 2:29:30, time: 6.885, data_time: 3.962, memory: 21614, loss: 0.0699
2023-11-13 16:08:15,296 - mmcls - INFO - Epoch [1][100/195]	lr: 2.636e-05, eta: 3 days, 3:59:26, time: 7.180, data_time: 0.216, memory: 21614, loss: 0.0058
2023-11-13 16:14:21,699 - mmcls - INFO - Epoch [1][150/195]	lr: 3.917e-05, eta: 3 days, 4:57:21, time: 7.328, data_time: 0.001, memory: 21614, loss: 0.0020
2023-11-13 16:22:09,381 - mmcls - INFO - Epoch [2][50/195]	lr: 6.350e-05, eta: 2 days, 2:53:03, time: 1.768, data_time: 0.880, memory: 21614, loss: 0.0011
2023-11-13 16:23:28,158 - mmcls - INFO - Epoch [2][100/195]	lr: 7.630e-05, eta: 1 day, 21:04:34, time: 1.576, data_time: 0.553, memory: 21614, loss: 0.0007
2023-11-13 16:24:51,309 - mmcls - INFO - Epoch [2][150/195]	lr: 8.910e-05, eta: 1 day, 17:04:53, time: 1.663, data_time: 0.385, memory: 21614, loss: 0.0008
2023-11-13 16:27:34,630 - mmcls - INFO - Epoch [3][50/195]	lr: 1.134e-04, eta: 1 day, 10:21:03, time: 1.823, data_time: 1.101, memory: 21614, loss: 0.0006
2023-11-13 16:28:55,153 - mmcls - INFO - Epoch [3][100/195]	lr: 1.262e-04, eta: 1 day, 8:33:48, time: 1.610, data_time: 0.894, memory: 21614, loss: 0.0008
2023-11-13 16:30:18,185 - mmcls - INFO - Epoch [3][150/195]	lr: 1.390e-04, eta: 1 day, 7:09:10, time: 1.661, data_time: 0.944, memory: 21614, loss: 0.0010
2023-11-13 16:32:56,265 - mmcls - INFO - Epoch [4][50/195]	lr: 1.633e-04, eta: 1 day, 3:53:44, time: 1.750, data_time: 0.685, memory: 21614, loss: 0.0005
2023-11-13 16:34:14,931 - mmcls - INFO - Epoch [4][100/195]	lr: 1.761e-04, eta: 1 day, 3:02:52, time: 1.573, data_time: 0.142, memory: 21614, loss: 0.0007
2023-11-13 16:35:34,529 - mmcls - INFO - Epoch [4][150/195]	lr: 1.889e-04, eta: 1 day, 2:19:34, time: 1.592, data_time: 0.001, memory: 21614, loss: 0.0003
2023-11-13 16:38:13,877 - mmcls - INFO - Epoch [5][50/195]	lr: 2.131e-04, eta: 1 day, 0:23:56, time: 1.791, data_time: 0.780, memory: 21614, loss: 0.0006
2023-11-13 16:39:31,685 - mmcls - INFO - Epoch [5][100/195]	lr: 2.259e-04, eta: 23:55:07, time: 1.556, data_time: 0.180, memory: 21614, loss: 0.0013
2023-11-13 16:40:52,776 - mmcls - INFO - Epoch [5][150/195]	lr: 2.386e-04, eta: 23:31:30, time: 1.622, data_time: 0.328, memory: 21614, loss: 0.0006
2023-11-13 16:43:31,417 - mmcls - INFO - Epoch [6][50/195]	lr: 2.629e-04, eta: 22:11:16, time: 1.742, data_time: 0.955, memory: 21614, loss: 0.0009
2023-11-13 16:44:49,567 - mmcls - INFO - Epoch [6][100/195]	lr: 2.756e-04, eta: 21:53:38, time: 1.563, data_time: 0.605, memory: 21614, loss: 0.0009
2023-11-13 16:46:12,640 - mmcls - INFO - Epoch [6][150/195]	lr: 2.883e-04, eta: 21:40:12, time: 1.661, data_time: 0.709, memory: 21614, loss: 0.0010
2023-11-13 16:48:55,698 - mmcls - INFO - Epoch [7][50/195]	lr: 3.125e-04, eta: 20:42:34, time: 1.806, data_time: 1.081, memory: 21614, loss: 0.0015
2023-11-13 16:50:16,310 - mmcls - INFO - Epoch [7][100/195]	lr: 3.252e-04, eta: 20:31:58, time: 1.612, data_time: 0.894, memory: 21614, loss: 0.0005
2023-11-13 16:51:36,216 - mmcls - INFO - Epoch [7][150/195]	lr: 3.379e-04, eta: 20:21:45, time: 1.598, data_time: 0.879, memory: 21614, loss: 0.0016
2023-11-13 16:54:13,624 - mmcls - INFO - Epoch [8][50/195]	lr: 3.620e-04, eta: 19:35:30, time: 1.746, data_time: 0.795, memory: 21614, loss: 0.0045
2023-11-13 16:55:31,551 - mmcls - INFO - Epoch [8][100/195]	lr: 3.747e-04, eta: 19:27:09, time: 1.559, data_time: 0.364, memory: 21614, loss: 0.0026
2023-11-13 16:56:52,624 - mmcls - INFO - Epoch [8][150/195]	lr: 3.874e-04, eta: 19:20:33, time: 1.621, data_time: 0.428, memory: 21614, loss: 0.0013
2023-11-13 16:59:30,285 - mmcls - INFO - Epoch [9][50/195]	lr: 4.114e-04, eta: 18:43:25, time: 1.763, data_time: 0.947, memory: 21614, loss: 0.0013
2023-11-13 17:00:48,085 - mmcls - INFO - Epoch [9][100/195]	lr: 4.241e-04, eta: 18:37:18, time: 1.556, data_time: 0.595, memory: 21614, loss: 0.0014
2023-11-13 17:02:09,001 - mmcls - INFO - Epoch [9][150/195]	lr: 4.367e-04, eta: 18:32:35, time: 1.618, data_time: 0.762, memory: 21614, loss: 0.0013
2023-11-13 17:04:49,417 - mmcls - INFO - Epoch [10][50/195]	lr: 4.607e-04, eta: 18:02:01, time: 1.787, data_time: 0.698, memory: 21614, loss: 0.0014
2023-11-13 17:06:07,179 - mmcls - INFO - Epoch [10][100/195]	lr: 4.733e-04, eta: 17:57:24, time: 1.555, data_time: 0.177, memory: 21614, loss: 0.0006
2023-11-13 17:07:28,043 - mmcls - INFO - Epoch [10][150/195]	lr: 4.859e-04, eta: 17:53:57, time: 1.617, data_time: 0.021, memory: 21614, loss: 0.0014
2023-11-13 17:08:38,137 - mmcls - INFO - Saving checkpoint at 10 epochs
2023-11-13 17:09:17,247 - mmcls - INFO - Epoch(val) [10][22]	accuracy_top-1: 99.8784, accuracy_top-2: 100.0000
2023-11-13 17:10:48,275 - mmcls - INFO - Epoch [11][50/195]	lr: 5.097e-04, eta: 17:28:23, time: 1.820, data_time: 1.102, memory: 21614, loss: 0.0056
2023-11-13 17:12:05,772 - mmcls - INFO - Epoch [11][100/195]	lr: 5.223e-04, eta: 17:24:42, time: 1.550, data_time: 0.780, memory: 21614, loss: 0.0029
2023-11-13 17:13:27,977 - mmcls - INFO - Epoch [11][150/195]	lr: 5.348e-04, eta: 17:22:32, time: 1.644, data_time: 0.913, memory: 21614, loss: 0.0020
2023-11-13 17:16:10,001 - mmcls - INFO - Epoch [12][50/195]	lr: 5.586e-04, eta: 16:59:50, time: 1.789, data_time: 1.022, memory: 21614, loss: 0.0011
2023-11-13 17:17:29,768 - mmcls - INFO - Epoch [12][100/195]	lr: 5.711e-04, eta: 16:57:32, time: 1.595, data_time: 0.587, memory: 21614, loss: 0.0009
2023-11-13 17:18:50,644 - mmcls - INFO - Epoch [12][150/195]	lr: 5.836e-04, eta: 16:55:34, time: 1.618, data_time: 0.751, memory: 21614, loss: 0.0016
2023-11-13 17:21:31,550 - mmcls - INFO - Epoch [13][50/195]	lr: 6.073e-04, eta: 16:35:16, time: 1.770, data_time: 0.982, memory: 21614, loss: 0.0022
2023-11-13 17:22:49,488 - mmcls - INFO - Epoch [13][100/195]	lr: 6.198e-04, eta: 16:33:00, time: 1.559, data_time: 0.836, memory: 21614, loss: 0.0022
2023-11-13 17:24:13,935 - mmcls - INFO - Epoch [13][150/195]	lr: 6.322e-04, eta: 16:32:22, time: 1.689, data_time: 0.964, memory: 21614, loss: 0.0017
2023-11-13 17:26:58,064 - mmcls - INFO - Epoch [14][50/195]	lr: 6.558e-04, eta: 16:14:52, time: 1.827, data_time: 0.782, memory: 21614, loss: 0.0014
2023-11-13 17:28:15,683 - mmcls - INFO - Epoch [14][100/195]	lr: 6.682e-04, eta: 16:12:54, time: 1.552, data_time: 0.557, memory: 21614, loss: 0.0014
2023-11-13 17:29:36,585 - mmcls - INFO - Epoch [14][150/195]	lr: 6.805e-04, eta: 16:11:43, time: 1.618, data_time: 0.757, memory: 21614, loss: 0.0013
2023-11-13 17:32:19,666 - mmcls - INFO - Epoch [15][50/195]	lr: 7.040e-04, eta: 15:55:35, time: 1.800, data_time: 0.982, memory: 21614, loss: 0.0022
2023-11-13 17:33:40,681 - mmcls - INFO - Epoch [15][100/195]	lr: 7.163e-04, eta: 15:54:40, time: 1.620, data_time: 0.592, memory: 21614, loss: 0.0024
2023-11-13 17:35:04,189 - mmcls - INFO - Epoch [15][150/195]	lr: 7.286e-04, eta: 15:54:15, time: 1.670, data_time: 0.670, memory: 21614, loss: 0.0016
2023-11-13 17:37:45,683 - mmcls - INFO - Epoch [16][50/195]	lr: 7.520e-04, eta: 15:39:24, time: 1.789, data_time: 0.959, memory: 21614, loss: 0.0017
2023-11-13 17:39:02,867 - mmcls - INFO - Epoch [16][100/195]	lr: 7.642e-04, eta: 15:37:53, time: 1.544, data_time: 0.579, memory: 21614, loss: 0.0016
2023-11-13 17:40:23,314 - mmcls - INFO - Epoch [16][150/195]	lr: 7.765e-04, eta: 15:37:01, time: 1.609, data_time: 0.644, memory: 21614, loss: 0.0009
2023-11-13 17:43:02,242 - mmcls - INFO - Epoch [17][50/195]	lr: 7.996e-04, eta: 15:23:06, time: 1.758, data_time: 0.621, memory: 21614, loss: 0.0010
2023-11-13 17:44:19,564 - mmcls - INFO - Epoch [17][100/195]	lr: 8.118e-04, eta: 15:21:49, time: 1.546, data_time: 0.072, memory: 21614, loss: 0.0008
2023-11-13 17:45:40,129 - mmcls - INFO - Epoch [17][150/195]	lr: 8.240e-04, eta: 15:21:08, time: 1.611, data_time: 0.028, memory: 21614, loss: 0.0016
2023-11-13 17:48:17,659 - mmcls - INFO - Epoch [18][50/195]	lr: 8.470e-04, eta: 15:08:21, time: 1.768, data_time: 0.800, memory: 21614, loss: 0.0006
2023-11-13 17:49:35,947 - mmcls - INFO - Epoch [18][100/195]	lr: 8.591e-04, eta: 15:07:23, time: 1.566, data_time: 0.156, memory: 21614, loss: 0.0010
2023-11-13 17:50:54,243 - mmcls - INFO - Epoch [18][150/195]	lr: 8.712e-04, eta: 15:06:25, time: 1.566, data_time: 0.001, memory: 21614, loss: 0.0009
2023-11-13 17:53:33,285 - mmcls - INFO - Epoch [19][50/195]	lr: 8.941e-04, eta: 14:54:31, time: 1.765, data_time: 0.835, memory: 21614, loss: 0.0011
2023-11-13 17:54:52,243 - mmcls - INFO - Epoch [19][100/195]	lr: 9.061e-04, eta: 14:53:47, time: 1.579, data_time: 0.775, memory: 21614, loss: 0.0018
2023-11-13 17:56:12,272 - mmcls - INFO - Epoch [19][150/195]	lr: 9.181e-04, eta: 14:53:12, time: 1.601, data_time: 0.859, memory: 21614, loss: 0.0022
2023-11-13 17:58:50,394 - mmcls - INFO - Epoch [20][50/195]	lr: 9.409e-04, eta: 14:42:04, time: 1.764, data_time: 0.930, memory: 21614, loss: 0.0025
2023-11-13 18:00:06,906 - mmcls - INFO - Epoch [20][100/195]	lr: 9.528e-04, eta: 14:41:02, time: 1.530, data_time: 0.693, memory: 21614, loss: 0.1488
2023-11-13 18:01:26,218 - mmcls - INFO - Epoch [20][150/195]	lr: 9.647e-04, eta: 14:40:26, time: 1.586, data_time: 0.739, memory: 21614, loss: 0.1312
2023-11-13 18:02:36,930 - mmcls - INFO - Saving checkpoint at 20 epochs
2023-11-13 18:10:30,819 - mmcls - INFO - Epoch(val) [20][22]	accuracy_top-1: 68.2719, accuracy_top-2: 100.0000
2023-11-13 18:11:58,980 - mmcls - INFO - Epoch [21][50/195]	lr: 9.750e-04, eta: 14:29:58, time: 1.763, data_time: 0.864, memory: 21614, loss: 0.1316
2023-11-13 18:13:17,454 - mmcls - INFO - Epoch [21][100/195]	lr: 9.744e-04, eta: 14:29:19, time: 1.570, data_time: 0.687, memory: 21614, loss: 0.1314
2023-11-13 18:14:38,407 - mmcls - INFO - Epoch [21][150/195]	lr: 9.738e-04, eta: 14:29:00, time: 1.619, data_time: 0.789, memory: 21614, loss: 0.1321
2023-11-13 18:17:16,255 - mmcls - INFO - Epoch [22][50/195]	lr: 9.725e-04, eta: 14:19:04, time: 1.755, data_time: 0.885, memory: 21614, loss: 0.1315
2023-11-13 18:18:34,671 - mmcls - INFO - Epoch [22][100/195]	lr: 9.719e-04, eta: 14:18:27, time: 1.568, data_time: 0.178, memory: 21614, loss: 0.1325
2023-11-13 18:19:55,592 - mmcls - INFO - Epoch [22][150/195]	lr: 9.712e-04, eta: 14:18:10, time: 1.618, data_time: 0.030, memory: 21614, loss: 0.1310
2023-11-13 18:22:33,404 - mmcls - INFO - Epoch [23][50/195]	lr: 9.699e-04, eta: 14:08:57, time: 1.783, data_time: 0.963, memory: 21614, loss: 0.1318
2023-11-13 18:23:52,569 - mmcls - INFO - Epoch [23][100/195]	lr: 9.692e-04, eta: 14:08:29, time: 1.583, data_time: 0.846, memory: 21614, loss: 0.1316
2023-11-13 18:25:12,418 - mmcls - INFO - Epoch [23][150/195]	lr: 9.685e-04, eta: 14:08:04, time: 1.597, data_time: 0.832, memory: 21614, loss: 0.1310
2023-11-13 18:27:50,751 - mmcls - INFO - Epoch [24][50/195]	lr: 9.672e-04, eta: 13:59:17, time: 1.778, data_time: 0.738, memory: 21614, loss: 0.1319
2023-11-13 18:29:09,976 - mmcls - INFO - Epoch [24][100/195]	lr: 9.665e-04, eta: 13:58:50, time: 1.585, data_time: 0.546, memory: 21614, loss: 0.1322
2023-11-13 18:30:32,497 - mmcls - INFO - Epoch [24][150/195]	lr: 9.657e-04, eta: 13:58:47, time: 1.650, data_time: 0.894, memory: 21614, loss: 0.1311
2023-11-13 18:33:10,474 - mmcls - INFO - Epoch [25][50/195]	lr: 9.643e-04, eta: 13:50:26, time: 1.785, data_time: 0.599, memory: 21614, loss: 0.1320
2023-11-13 18:34:28,958 - mmcls - INFO - Epoch [25][100/195]	lr: 9.636e-04, eta: 13:49:55, time: 1.570, data_time: 0.027, memory: 21614, loss: 0.1310
2023-11-13 18:35:50,092 - mmcls - INFO - Epoch [25][150/195]	lr: 9.628e-04, eta: 13:49:41, time: 1.623, data_time: 0.001, memory: 21614, loss: 0.1315
